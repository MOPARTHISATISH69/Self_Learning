{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"8E_F_LR_SVM.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5HExLQrE4ZxR","colab_type":"text"},"source":["<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"]},{"cell_type":"markdown","metadata":{"id":"4LuKrFzC4ZxV","colab_type":"text"},"source":["<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"1wES-wWN4ZxX","colab_type":"text"},"source":["<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n","\n","Check the documentation for better understanding of these attributes: \n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","<img src='https://i.imgur.com/K11msU4.png' width=500>\n","\n","As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n","\n","Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n","\n","Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n","\n","Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n","$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n","\n","RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n","\n","For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"z830CfMk4Zxa","colab_type":"text"},"source":["## Task E"]},{"cell_type":"markdown","metadata":{"id":"MuBxHiCQ4Zxc","colab_type":"text"},"source":["> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n","\n","> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n","\n","> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"]},{"cell_type":"code","metadata":{"id":"ASzT7XCD-6Ec","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0JGYgfz_Z0W","colab_type":"code","colab":{}},"source":["X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9etCxaA7_NWi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1592574649742,"user_tz":-330,"elapsed":1035,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"f7024f0c-40de-42da-dff3-22f59e455d44"},"source":["from sklearn.model_selection import train_test_split\n","X_tr,X_test,y_tr,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n","X_train,X_cv,y_train,y_cv = train_test_split(X_tr,y_tr,test_size=0.2,random_state=24)\n","print(X_train.shape, y_train.shape)\n","print(X_cv.shape, y_cv.shape)\n","print(X_test.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3200, 5) (3200,)\n","(800, 5) (800,)\n","(1000, 5) (1000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tHie1zqH4Zxt","colab_type":"text"},"source":["### Pseudo code\n","\n","clf = SVC(gamma=0.001, C=100.)<br>\n","clf.fit(Xtrain, ytrain)\n","\n","<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n","   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n","    \n","fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n","\n","<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"]},{"cell_type":"code","metadata":{"id":"zdbr7Slb_scR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592574655339,"user_tz":-330,"elapsed":1043,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"b94d61eb-e331-4478-aa33-8fd47a9b6cf9"},"source":["gamma=0.001\n","clf = SVC(gamma=gamma, C=100)\n","clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"pRskdYBqAAki","colab_type":"code","colab":{}},"source":["def K(xq):\n","    val = 0\n","    for alpha,xi in zip(clf.dual_coef_[0],clf.support_vectors_): #the dual_coef_[i] contains label[i]*alpha[i]\n","        val += alpha*np.exp(-gamma*np.linalg.norm(xi-xq)**2) \n","    return val+clf.intercept_.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5IW1TppADi6","colab_type":"code","colab":{}},"source":["def dec_fun(X_cv):\n","    fcv = []\n","    for xq in X_cv:\n","        fcv.append(K(xq))\n","    fcv = np.array(fcv)\n","    return fcv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mnJwQw2AJ_3","colab_type":"code","colab":{}},"source":["fcv = dec_fun(X_cv)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CP6eegf9SCl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592574674520,"user_tz":-330,"elapsed":1210,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"b9d9a083-1c1a-4e97-e3f7-ac1905c64f01"},"source":["print(fcv[5:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-2.30501509  1.41770579 -0.43831055 -2.43467245  1.64358234]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AqYZhgmNAbOC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592574677306,"user_tz":-330,"elapsed":1190,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"e02b8aef-79a9-4a1a-e5ff-5351b83cddfc"},"source":["clf.decision_function(X_cv)[5:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-2.30501509,  1.41770579, -0.43831055, -2.43467245,  1.64358234])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"5iW1IkSBAgZ1","colab_type":"text"},"source":["We observe that the custom function gives same values as the inbuilt decision function."]},{"cell_type":"markdown","metadata":{"id":"c0bKCboN4Zxu","colab_type":"text"},"source":["<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"nMn7OEN94Zxw","colab_type":"text"},"source":["Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n","<img src='https://i.imgur.com/CAMnVnh.png'>\n"]},{"cell_type":"markdown","metadata":{"id":"e0n5EFkx4Zxz","colab_type":"text"},"source":["## TASK F"]},{"cell_type":"markdown","metadata":{"id":"t0HOqVJq4Zx1","colab_type":"text"},"source":["\n","> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n","\n","> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n","\n","> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n","<img src='https://i.imgur.com/zKYE9Oc.png'>\n","if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n","\n","> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"]},{"cell_type":"code","metadata":{"id":"t75nuaYQ6hg3","colab_type":"code","colab":{}},"source":["def initialize_weights(dim):\n","    ''' In this function, we will initialize our weights and bias'''\n","    #initialize the weights to zeros array of (dim,1) dimensions\n","    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n","    #initialize bias to zero\n","    w = np.zeros_like(dim)\n","    b = 0\n","\n","    return w,b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxLCq3dv7FFT","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    ''' In this function, we will return sigmoid of z'''\n","    # compute sigmoid(z) and return\n","\n","    return 1 / (1 + np.exp(-z))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0FtqIx47IEO","colab_type":"code","colab":{}},"source":["def logloss(w, b, x, y):\n","    '''In this function, we will compute log loss '''\n","    N = len(y)\n","    sum_log = 0\n","    for i in range(N):\n","        sum_log += y[i] * np.log10(sigmoid(np.dot(w, x[i]) + b)) + (1 - y[i]) * np.log10(1 - sigmoid(np.dot(w, x[i]) + b))\n","    return -1 * sum_log/N"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7L1k2xBI7KLm","colab_type":"code","colab":{}},"source":["def gradient_dw(x,y,w,b,alpha,N):\n","    '''In this function, we will compute the gardient w.r.to w '''\n","    \n","    dw = x * (y - sigmoid(np.dot(w,x) + b) - (alpha / N) * w)\n","            \n","    return dw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vrKQfCH7NT5","colab_type":"code","colab":{}},"source":[" def gradient_db(x,y,w,b):\n","     '''In this function, we will compute gradient w.r.to b '''\n","\n","     db = y - sigmoid(np.dot(w,x) + b)\n","    \n","     return db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjAMXEy77S3w","colab_type":"code","colab":{}},"source":["## https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray-in-python\n","def converter(y_cv):\n","    l = []\n","    unique, counts = np.unique(y_cv, return_counts=True)\n","    d = dict(zip(unique, counts))\n","    \n","    p_plus = (d[1] + 1) / (d[1] + 2)\n","    p_minus = 1 / (d[0] - 1)\n","    \n","    \n","    for i in y_cv:\n","        if i == 0:\n","            l.append(p_minus)\n","        else:\n","            l.append(p_plus)\n","    ycv = np.array(l)\n","    return ycv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpRMd4r47jYj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592574711527,"user_tz":-330,"elapsed":959,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"f9b4ced7-5ded-443f-ca5e-016c6fc31321"},"source":["ycv = converter(y_cv)\n","print(len(ycv))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RONKjQ5d7rsy","colab_type":"code","colab":{}},"source":["def train(fcv,ycv,epochs,alpha,eta0):\n","    test_loss = []\n","    w, b = initialize_weights(fcv[0])\n","    for i in range(epochs):\n","        for j in range(N):\n","            dw = gradient_dw(fcv[j],ycv[j],w,b,alpha,N)\n","            db = gradient_db(fcv[j],ycv[j],w,b)\n","            w = w + (eta0 * dw)\n","            b = b + (eta0 * db)  \n","        loss = logloss(w,b,fcv,ycv)\n","        test_loss.append(loss)\n","    return w,b,test_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B9RaBcE87vn-","colab_type":"code","colab":{}},"source":["alpha=0.0001\n","eta0=0.0001\n","N=len(fcv)\n","epochs=50\n","w,b,test_loss=train(fcv,ycv,epochs,alpha,eta0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvYfsCbp9X9X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592574724104,"user_tz":-330,"elapsed":963,"user":{"displayName":"Satishkumar Moparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUSOPEhegxKhmSJgOTfyqrLlGWk7dHBnuvO3S-gA=s64","userId":"03515051603858730688"}},"outputId":"652dcb5d-a428-4d78-8631-6e0c3e38ce1c"},"source":["print(\"Updated Weight = \",w,\" and updated intercept =\",b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Updated Weight =  1.135848495170214  and updated intercept = -0.09358559976954377\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oTY7z2bd4Zx2","colab_type":"text"},"source":["__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"]},{"cell_type":"markdown","metadata":{"id":"CM3odN1Z4Zx3","colab_type":"text"},"source":["\n","If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n","\n","1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n","\n","2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n","\n","3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n","\n","4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"]}]}